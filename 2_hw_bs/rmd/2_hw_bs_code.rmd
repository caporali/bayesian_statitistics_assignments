---
title: "Assignment 2"
author: "Francesco Caporali, Isabel Muzio"
date: '`r Sys.setlocale("LC_TIME", "English"); format(Sys.time(), "%d %B %Y")`'
output:
    pdf_document:
        latex_engine: pdflatex
        includes:
            in_header: files/preamble.tex
        extra_dependencies:
            inputenc: ["utf8"]
            fontenc: ["T1"]
            dsfont: null
            cancel: null
            mathtools: null
            amsmath: null
            amsfonts: null
            amsthm: null
    html_document: default
documentclass: article
fontsize: 11pt
geometry: margin = 2cm
header-includes:
- \newcommand{\indicator}{\mathds{1}}
- \newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
- \newcommand{\var}[1]{\operatorname{Var}\left(#1\right)}
- \newcommand{\cov}[2]{\operatorname{Cov}\left(#1,#2\right)}
- \newcommand{\corr}[2]{\operatorname{Corr}\left(#1,#2\right)}
- \renewcommand{\det}[1]{\operatorname{det}\left(#1\right)}
- \newcommand{\real}{\mathbb{R}}
- \newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
- \newcommand{\deq}{\stackrel{\text{def}}{=}}
- \newcommand{\given}{\,|\,}
- \newcommand{\convp}{\xrightarrow{\prob}}
- \newcommand{\simiid}{\stackrel{\mathclap{\normalfont\mbox{\scriptsize{iid}}}}{\sim}}
- \renewcommand{\epsilon}{\varepsilon}
- \newcommand{\ts}[1]{\textsuperscript{#1}}
- \renewcommand{\labelitemi}{\normalfont\bfseries\textendash}
- \newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}
- \newcommand{\truncnormal}[3]{\mathcal{T}\mathcal{N}_{#1}\left(#2,#3\right)}
- \newcommand{\bernoulli}[1]{\operatorname{Bernoulli}\left(#1\right)}
- \renewcommand{\exp}[1]{\operatorname{exp}\left(#1\right)}
- \newcommand{\qed}{\hfill \ensuremath{\Box}}
---
```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)

colorize <- function(x, color) {
    if (knitr::is_latex_output())
        sprintf("\\textcolor{%s}{%s}", color, x)
    else if (knitr::is_html_output())
        sprintf('<span style = "color: %s;">%s</span>', color, x)
    else x
} # `r textit("red", "aaa")`
indicator <- function() {
    if (knitr::is_latex_output())
        sprintf("\\indicator")
    else if (knitr::is_html_output())
        sprintf("1")
    else sprintf("1")
} #`r indicator()`
simiid <- function() {
    if (knitr::is_latex_output())
        sprintf("\\simiid")
    else if (knitr::is_html_output())
        sprintf("\\sim")
    else sprintf("\\sim")
} #`r simiid()`
newline <- function() {
    if (knitr::is_latex_output())
        sprintf("\\")
    else if (knitr::is_html_output())
        sprintf("<br>")
    else ""
} # `r newline()`
textit <- function(arg) {
    if (knitr::is_latex_output())
        sprintf("\\textit{%s}", arg)
    else if (knitr::is_html_output())
        sprintf("<i>%s</i>", arg)
    else sprintf("%s", arg)
} # `r textit("aaa")`
textbf <- function(arg) {
    if (knitr::is_latex_output())
        sprintf("\\textbf{%s}", arg)
    else if (knitr::is_html_output())
        sprintf("<strong>%s</strong>", arg)
    else sprintf("%s", arg)
} # `r textbf("aaa")`
begin_rcases <- function() {
    if (knitr::is_latex_output())
        sprintf("\\begin{rcases}")
    else if (knitr::is_html_output())
        sprintf("\\begin{array}{l}")
    else ""
} # `r begin_rcases()`
end_rcases <- function() {
    if (knitr::is_latex_output())
        sprintf("\\end{rcases}")
    else if (knitr::is_html_output())
        sprintf("\\end{array} \\hspace{0.3cm}")
    else ""
} # `r end_rcases()`
cancel <- function(x) {
    if (knitr::is_latex_output())
        sprintf("\\cancel{%s}", x)
    else if (knitr::is_html_output())
        sprintf("%s", x)
    else sprintf("%s", x)
} # `r cancel("x")`

library(ggplot2)
```

# Question 1: Probit regression (Hoff 6.3)

A panel study followed $n = 25$ married couples over a period of five years. One item of interest is the relationship between divorce rates and the various characteristics of the couples. For example, the researchers would like to model the probability of divorce as a function of age differential, recorded as the man's age minus the woman's age. The data can be found in the file \texttt{divorce.RData}. We will model these data with probit regression, in which a binary variable $Y_i$ is described in terms of an explanatory variable $x_i$ via the following latent variable model: 
\begin{align*}
    Z_i & = \beta x_i + \epsilon_i \\
    Y_i & = `r indicator()`_{(c, +\infty)}(Z_i),
\end{align*}
where $\beta$ and $c$ are unknown coefficients, $\epsilon_1, \dots, \epsilon_n `r simiid()` \normal{0}{1}$ and $`r indicator()`_{(c, +\infty)}(z) = 1$ if $z > c$ and equals zero otherwise. In the following, since the covariates $x_i$ are known, they will be treated as constants and so not
explicitly written in the conditioning part.

## Point a.

Assuming $\beta \sim \normal{0}{\sigma_\beta^2}$, obtain the full conditional distribution $p(\beta \given y_{1:n}, z_{1:n}, c)$. 

\medskip

First of all let us write explicitly the conditional distributions which we can deduce from the text:

- $\forall i = 1, \dots, n$ we know $p(z_i \given \beta)$: 
    \begin{gather*}
        Z_i(\omega) \given \beta = \beta x_i + \epsilon_i(\omega) \sim \beta x_i + \normal{0}{1} \sim \normal{\beta x_i}{1} \implies Z_i \given \beta \sim \normal{\beta x_i}{1} \\
        \Downarrow \\
        p(z_i \given \beta) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (z_i - x_i \beta)^2};
    \end{gather*}
- $\forall i = 1, \dots, n$ we know $p(y_i \given c, z_i)$: 
    \begin{gather*}
        Y_i(\omega) = `r indicator()`_{(c, +\infty)}(Z_i(\omega)) = 
        \begin{cases}
            1 & \text{ if } Z_i(\omega) > c \\
            0 & \text{ otherwise}
        \end{cases} \\
        \Downarrow \\
        \begin{aligned}
            p(y_i) & = \prob{Y_i = y_i} = \prob{`r indicator()`_{(c, +\infty)}(Z_i) = y_i} = \\
            & = \begin{cases}
                    \prob{`r indicator()`_{(c, +\infty)}(Z_i) = 1} & \text{ if } y_i = 1 \\
                    \prob{`r indicator()`_{(c, +\infty)}(Z_i) = 0} & \text{ if } y_i = 0 \\
                    0 & \text{ otherwise}
                \end{cases} = \\
            & = \begin{cases}
                    \prob{\{Z_i > c\}} & \text{ if } y_i = 1 \\
                    \prob{\{Z_i > c\}^C} & \text{ if } y_i = 0 \\
                    0 & \text{ otherwise}
                \end{cases} = \\
            & = \left(y_i \prob{\{Z_i > c\}} + (1 - y_i) \prob{\{Z_i > c\}^C}\right) `r indicator()`_{\{0, 1\}}(y_i),
        \end{aligned}
    \end{gather*}
    hence $Y_i \sim \bernoulli{\prob{Z_i > c}}$. `r newline()`
    It follows that, conditionally on $Z_i, c$, the r.v. $Y_i$ is no more `r textit("random")` and it holds[^1]
    \[
        p(y_i \given c, z_i) = \left(y_i `r indicator()`_{(-\infty, z_i)}(c) + (1 - y_i) `r indicator()`_{(-\infty, z_i)^C}(c)\right) `r indicator()`_{\{0, 1\}}(y_i).
    \]

    [^1]: We replace $\prob{\{z_i > c\}}$ with $`r indicator()`_{(-\infty, z_i)}(c)$ because we will use this characterization afterwards.

In order to obtain (and sample) from the full conditionals we assume $\beta$ and $c$ a priori independent. `r newline()`
The full conditional distribution $p(\beta \given y_{1:n}, z_{1:n}, c)$ can be obtained just from $p(z_i \given \beta)$, indeed
    \begin{align*}
        p(\beta \given y_{1:n}, z_{1:n}, c) & = \frac{p(\beta, y_{1:n}, z_{1:n}, c)}{p(y_{1:n}, z_{1:n}, c)} \frac{p(\beta, z_{1:n}, c)}{p(\beta, z_{1:n}, c)} \frac{p(\beta, c)}{p(\beta, c)} \frac{p(c)}{p(c)} \propto \\
        & \propto \frac{p(\beta, y_{1:n}, z_{1:n}, c)}{p(\beta, z_{1:n}, c)} \frac{p(\beta, z_{1:n}, c)}{p(\beta, c)} \frac{p(\beta, c)}{p(c)} = \\
        & = p(y_{1:n} \given `r cancel("\\beta")`, c, z_{1:n}) p(z_{1:n} \given \beta, `r cancel("c")`) p(\beta \given `r cancel("c")`) \propto \\
        & \propto p(z_{1:n} \given \beta) p(\beta).
    \end{align*}
So we can write explicitly
    \begin{align*}
        p(\beta \given y_{1:n}, z_{1:n}, c) & \propto p(z_{1:n} \given \beta) p(\beta) = \\
        & = \prod_{i = 1}^n p(z_i \given \beta) p(\beta) \propto \\
        & \propto \exp{-\frac{1}{2} \sum_{i = 1}^{n} (z_i - x_i \beta)^2} \exp{-\frac{1}{2} \frac{1}{\sigma_\beta^2} \beta^2} = \\
        & = \exp{-\frac{1}{2} \left(\beta^2 \sum_{i = 1}^{n} x_i^2 + `r cancel("\\sum_{i = 1}^{n} z_i^2")` - 2 \beta \sum_{i = 1}^{n} x_i z_i + \beta^2 \frac{1}{\sigma_\beta^2} \right)} = \\
        & = \operatorname{exp}\Bigg(- \underbrace{\left(\sum_{i = 1}^{n} x_i^2 + \frac{1}{\sigma_\beta^2}\right)}_{\displaystyle \deq (\sigma_{\beta, n}^2)^{-1}} \frac{\beta^2}{2} + \underbrace{\left(\sum_{i = 1}^{n} x_i z_i \right)}_{\displaystyle \deq \frac{\mu_{\beta, n}}{\sigma_{\beta, n}^2}} \beta \Bigg) ,
    \end{align*}
where from the 1\ts{st} to the 2\ts{nd} line we used $\left(Z_i \given \beta\right)_{i = 1}^n$ independent, identically distributed r.v.'s. `r newline()`
So we can conclude that 
    \begin{gather*}
        \beta \given y_{1:n}, z_{1:n}, c \sim \normal{\mu_{\beta, n}}{\sigma_{\beta, n}^2} \text{ with } 
        \begin{cases}
            \sigma_{\beta, n}^2 = \left(\sum_{i = 1}^{n} x_i^2 + \frac{1}{\sigma_\beta^2}\right)^{-1} \\
            \mu_{\beta, n} = \sigma_{\beta, n}^2 \left(\sum_{i = 1}^{n} x_i z_i \right)
        \end{cases} \\
        \Downarrow \\
        p(\beta \given y_{1:n}, z_{1:n}, c) = \frac{1}{\sqrt{2\pi\sigma_{\beta, n}^2}} \exp{-\frac{1}{2 \sigma_{\beta, n}^2} (\beta - \mu_{\beta, n})^2}.
    \end{gather*}
\qed

## Point b.

Assuming $c \sim \normal{0}{\sigma_c^2}$, show that $p(c \given y_{1:n}, z_{1:n}, \beta)$ is a constrained normal density, i.e. proportional to a normal density but constrained to lie in an interval. Similarly, show that $p(z_i \given y_{1:n}, z_{-i}, \beta, c)$ is proportional to a normal density but constrained to be either above $c$ or below $c$, depending on $y_i$. `r newline()`
`r textbf("Hint:")` A constrained, or truncated, normal random variable $V$ is obtained by restricting a normally distributed random variable $\normal{\mu}{\tau^2}$ to lie in an interval $(a, b)$, with possibly $a = -\infty$ or $b = +\infty$. We use the notation $V \sim \truncnormal{(a,b)}{\mu}{\tau^2}$. It holds:

- $p(v \given \mu, \tau^2, a, b) = \frac{1}{C} \frac{1}{\sqrt{2\pi\tau^2}} \exp{-\frac{1}{2\tau^2}(v - \mu)^2} `r indicator()`_{(a, b)}(v)$, where $C = \Phi\left(\frac{b - \mu}{\tau}\right) - \Phi\left(\frac{a - \mu}{\tau}\right)$ being $\Phi(\cdot)$ the cdf of the standard normal distribution. By definition, it holds $\Phi\left(\frac{b - \mu}{\tau}\right) = 1$ if $b = \infty$ and $\Phi\left(\frac{a - \mu}{\tau}\right) = 0$ if $a = -\infty$.
- Sampling can be performed thanks to the function \texttt{rtruncnorm(n, a, b, mean, sd)} from the package \texttt{rtruncnorm} [https://cran.r-project.org/web/packages/truncnorm/truncnorm.pdf]. This function receives in input the number of desired samples ($n$) and the four parameters specifying the distribution of $V$: $a, b, \mu, \tau$. Pay attention that it takes as last inputs the mean $\mu$ and the standard deviation $\tau$ (not the variance $\tau^2$) of the un-truncated normal density.

\medskip

As before, the full conditional distribution $p(c \given y_{1:n}, z_{1:n}, \beta)$ can be obtained just from $p(y_i \given c, z_i)$, indeed
    \begin{align*}
		p(c \given y_{1:n}, z_{1:n}, \beta) & = \frac{p(c, y_{1:n}, z_{1:n}, \beta)}{p(y_{1:n}, z_{1:n}, \beta)} \frac{p(\beta, c, z_{1:n})}{p(\beta, c, z_{1:n})} \frac{p(c, \beta)}{p(c, \beta)} \frac{p(\beta)}{p(\beta)} \propto \\
        & \propto \frac{p(c, y_{1:n}, z_{1:n}, \beta)}{p(\beta, c, z_{1:n})} \frac{p(\beta, c, z_{1:n})}{p(c, \beta)} \frac{p(c, \beta)}{p(\beta)} = \\
        & = p(y_{1:n} \given `r cancel("\\beta")`, c, z_{1:n}) p(z_{1:n} \given \beta, `r cancel("c")`) p(c \given `r cancel("\\beta")`) \propto \\
        & \propto p(y_{1:n} \given c, z_{1:n}) p(c).
    \end{align*}
So we can write explicitly
    \begin{align*}
        p(c \given y_{1:n}, z_{1:n}, \beta) & \propto p(y_{1:n} \given c, z_{1:n}) p(c) = \\
        & = \prod_{i = 1}^n p(y_i \given c, z_i) p(c) \propto \\
        & \propto \exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} \prod_{i = 1}^n \left(y_i `r indicator()`_{(-\infty, z_i)}(c) + (1 - y_i) `r indicator()`_{(-\infty, z_i)^C}(c)\right) `r indicator()`_{\{0, 1\}}(y_i) = \\
		& = \exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} \prod_{{i = 1, \dots, n \given y_i = 1}} `r indicator()`_{(-\infty, z_i)}(c) \cdot \prod_{{i = 1, \dots, n \given y_i = 0}} `r indicator()`_{[z_i, +\infty)}(c) = \\
		& = \exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} `r indicator()`_{(-\infty, \, \min\left(z_i \given i \in \{1, \dots, n\}, y_i = 1\right))}(c) `r indicator()`_{[\max\left(z_i \given i \in \{1, \dots, n\}, y_i = 0\right), \, +\infty)}(c),
    \end{align*}
where from the 1\ts{st} to the 2\ts{nd} line we used $\left(Y_i \given c, z_i\right)_{i = 1}^n$ independent, identically distributed r.v.'s. `r newline()`
More compactly, defining 
	\begin{align*}
		a_n & \deq \max\left(z_i \given i \in \{1, \dots, n\}, y_i = 0\right) \text{ and } \\
		b_n & \deq \min\left(z_i \given i \in \{1, \dots, n\}, y_i = 1\right),
	\end{align*}
one has 
	\[
        p(c \given y_{1:n}, z_{1:n}, \beta) \propto \exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} `r indicator()`_{[a_n, b_n)}(c),
	\]
where, for a good definition, we are using $a_n < b_n$ which is clearly true because, if $a_n, b_n$ are finite, $\forall i, j \in \{1, \dots, n\}$ such that $y_i = 0$, $y_j = 1$, $(-\infty, c] \ni z_i < z_j \in (c, +\infty)$. `r newline()`
First of all we have to observe that the indicator function constrains $c \in [a_n, b_n)$, but it is equivalent to $c \in (a_n, b_n)$ because our $p(c \given y_{1:n}, z_{1:n}, \beta)$ is a density function with respect to the lebesgue measure on $\real$ so each point has measure $0$ (so does $\{a_n\}$). `r newline()`
Then, let us observe that this conditional density is proportional to the kernel of a gaussian (evaluated in $c$) multiplied by an indicator function (also evaluated in $c$), which constrains the domain to an interval (not necessarily limited, possibly $a_n = -\infty$ or $b_n = + \infty$). `r newline()`
So completing the function $\exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} `r indicator()`_{(a_n, b_n)}(c)$ to a density one obtains
	\begin{gather*}
		p(c \given y_{1:n}, z_{1:n}, \beta) = \frac{1}{\displaystyle \Phi\left(\frac{b_n}{\sigma_c}\right) - \Phi\left(\frac{a_n}{\sigma_c}\right)} \frac{1}{\sqrt{2 \pi \sigma_c^2}} \exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} `r indicator()`_{(a_n, b_n)}(c) \\
		\Downarrow \\
		c \given y_{1:n}, z_{1:n}, \beta \sim \truncnormal{(a_n, b_n)}{0}{\sigma_c^2}.
	\end{gather*}

\smallskip 

Similarly
	\begin{align*}
		p(z_i \given y_{1:n}, z_{-i}, \beta, c) & = \frac{p(z_i, y_{1:n}, z_{-i}, \beta, c)}{p(y_{1:n}, z_{-i}, \beta, c)} \frac{p(z_i, z_{-i}, \beta, c)}{p(z_i, z_{-i}, \beta, c)} \frac{p(z_i, \beta, c)}{p(z_i, \beta, c)} \frac{p(\beta, c)}{p(\beta, c)} \propto \\
		& \propto \frac{p(z_i, y_{1:n}, z_{-i}, \beta, c)}{p(z_i, z_{-i}, \beta, c)} \frac{p(z_i, z_{-i}, \beta, c)}{p(z_i, \beta, c)} \frac{p(z_i, \beta, c)}{p(\beta, c)} = \\
		& = p(y_{1:n} \given z_{1:n}, `r cancel("\\beta")`, c) p(z_{-i} \given `r cancel("z_i")`, \beta, `r cancel("c")`) p(z_i \given \beta, `r cancel("c")`) \propto \\
		& \propto p(y_{1:n} \given z_{1:n}, c) p(z_i \given \beta) \propto \\
		& \propto \prod_{j = 1}^n p(y_j \given z_j, c) p(z_i \given \beta) \propto \\
		& \propto p(y_i \given z_i, c) p(z_i \given \beta) \propto \\
		& \propto \Big(y_i \underbrace{`r indicator()`_{(-\infty, z_i)}(c)}_{\displaystyle = `r indicator()`_{(c, +\infty)}(z_i)} + (1 - y_i) \underbrace{`r indicator()`_{(-\infty, z_i)^C}(c)}_{\displaystyle = `r indicator()`_{(-\infty, c]}(z_i)}\Big) `r indicator()`_{\{0, 1\}}(y_i) \exp{-\frac{1}{2} (z_i - x_i \beta)^2} = \\
		& = 
			\begin{cases}
				\displaystyle `r indicator()`_{(c, +\infty)}(z_i) \exp{-\frac{1}{2} (z_i - x_i \beta)^2} & \text{ if } y_i = 1 \\
				\displaystyle `r indicator()`_{(-\infty, c]}(z_i) \exp{-\frac{1}{2} (z_i - x_i \beta)^2} & \text{ if } y_i = 0 \\
			\end{cases}.
	\end{align*}
As before, this conditional density is proportional to the kernel of a gaussian (evaluated in $z_i$) multiplied by an indicator function (also evaluated in $z_i$) which constrains the domain to be $(c, +\infty)$ or $(-\infty, c]$ (equivalently $(-\infty, c)$, with the same motivation given above) depending on $y_i$. `r newline()`
In particular, completing to a density what we found
	\begin{gather*}
		p(z_i \given y_{1:n}, z_{-i}, \beta, c) = 
		\begin{cases}
			\displaystyle \frac{1}{1 - \Phi(c - x_i\beta)} \frac{1}{\sqrt{2 \pi}} \exp{-\frac{1}{2} (z_i - x_i \beta)^2} `r indicator()`_{(c, +\infty)}(z_i) & \text{ if } y_i = 1 \\
			\displaystyle \frac{1}{\Phi(c - x_i\beta)} \frac{1}{\sqrt{2 \pi}} \exp{-\frac{1}{2} (z_i - x_i \beta)^2} `r indicator()`_{(-\infty, c)}(z_i) & \text{ if } y_i = 0 \\
		\end{cases} \\
		\Downarrow \\
		Z_i \given y_{1:n}, z_{-i}, \beta, c \sim 
		\begin{cases}
			\displaystyle \truncnormal{(c, +\infty)}{x_i \beta}{1} \text{ if } y_i = 1 \\
			\displaystyle\truncnormal{(-\infty, c)}{x_i \beta}{1} \text{ if } y_i = 0 \\
		\end{cases}.
	\end{gather*}
\qed

## Point c.

Letting $\sigma_\beta^2 = \sigma_c^2 = 16$, implement a Gibbs sampling scheme that approximates the joint posterior distribution of $Z_{1:n}, \beta$ and $c$. After a burnin of $1000$, run the Gibbs sampler long enough so that the effective sample sizes of all unknown parameters are greater than $1000$ (including the $Z_i$'s).
Compute the autocorrelation function of the parameters and discuss the mixing of the Markov chain.

## Point d.

Obtain a $95\%$ posterior credible interval for $\beta$, as well as $\prob{\beta > 0 \given y_{1:n}}$.

\scriptsize
```{r}

```
\normalsize