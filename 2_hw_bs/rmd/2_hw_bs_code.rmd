---
title: "Assignment 2"
author: "Francesco Caporali, Isabel Muzio"
date: '`r Sys.setlocale("LC_TIME", "English"); format(Sys.time(), "%d %B %Y")`'
output:
    html_document: default
    pdf_document:
        latex_engine: pdflatex
        includes:
            in_header: files/preamble.tex
        extra_dependencies:
            inputenc: ["utf8"]
            fontenc: ["T1"]
            dsfont: null
            cancel: null
            mathtools: null
            amsmath: null
            amsfonts: null
            amsthm: null
documentclass: article
fontsize: 11pt
geometry: margin = 2cm
header-includes:
- \newcommand{\indicator}{\mathds{1}}
- \newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
- \newcommand{\var}[1]{\operatorname{Var}\left(#1\right)}
- \newcommand{\cov}[2]{\operatorname{Cov}\left(#1,#2\right)}
- \newcommand{\corr}[2]{\operatorname{Corr}\left(#1,#2\right)}
- \renewcommand{\det}[1]{\operatorname{det}\left(#1\right)}
- \newcommand{\real}{\mathbb{R}}
- \newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
- \newcommand{\deq}{\stackrel{\text{def}}{=}}
- \newcommand{\given}{\,|\,}
- \newcommand{\convp}{\xrightarrow{\prob}}
- \newcommand{\simiid}{\stackrel{\mathclap{\normalfont\mbox{\scriptsize{iid}}}}{\sim}}
- \renewcommand{\epsilon}{\varepsilon}
- \newcommand{\ts}[1]{\textsuperscript{#1}}
- \renewcommand{\labelitemi}{\normalfont\bfseries\textendash}
- \newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}
- \newcommand{\truncnormal}[3]{\mathcal{T}\mathcal{N}_{#1}\left(#2,#3\right)}
- \newcommand{\bernoulli}[1]{\operatorname{Bernoulli}\left(#1\right)}
- \renewcommand{\exp}[1]{\operatorname{exp}\left(#1\right)}
- \newcommand{\qed}{\hfill \ensuremath{\Box}}
---
```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)

colorize <- function(x, color) {
    if (knitr::is_latex_output())
        sprintf("\\textcolor{%s}{%s}", color, x)
    else if (knitr::is_html_output())
        sprintf('<span style = "color: %s;">%s</span>', color, x)
    else x
} # `r textit("red", "aaa")`
indicator <- function() {
    if (knitr::is_latex_output())
        sprintf("\\indicator")
    else if (knitr::is_html_output())
        sprintf("1")
    else sprintf("1")
} #`r indicator()`
simiid <- function() {
    if (knitr::is_latex_output())
        sprintf("\\simiid")
    else if (knitr::is_html_output())
        sprintf("\\sim")
    else sprintf("\\sim")
} #`r simiid()`
newline <- function() {
    if (knitr::is_latex_output())
        sprintf("\\")
    else if (knitr::is_html_output())
        sprintf("<br>")
    else ""
} # `r newline()`
textit <- function(arg) {
    if (knitr::is_latex_output())
        sprintf("\\textit{%s}", arg)
    else if (knitr::is_html_output())
        sprintf("<i>%s</i>", arg)
    else sprintf("%s", arg)
} # `r textit("aaa")`
textbf <- function(arg) {
    if (knitr::is_latex_output())
        sprintf("\\textbf{%s}", arg)
    else if (knitr::is_html_output())
        sprintf("<strong>%s</strong>", arg)
    else sprintf("%s", arg)
} # `r textbf("aaa")`
begin_rcases <- function() {
    if (knitr::is_latex_output())
        sprintf("\\begin{rcases}")
    else if (knitr::is_html_output())
        sprintf("\\begin{array}{l}")
    else ""
} # `r begin_rcases()`
end_rcases <- function() {
    if (knitr::is_latex_output())
        sprintf("\\end{rcases}")
    else if (knitr::is_html_output())
        sprintf("\\end{array} \\hspace{0.3cm}")
    else ""
} # `r end_rcases()`
cancel <- function(x) {
    if (knitr::is_latex_output())
        sprintf("\\cancel{%s}", x)
    else if (knitr::is_html_output())
        sprintf("%s", x)
    else sprintf("%s", x)
} # `r cancel("x")`

library(ggplot2)
```

# Question 1: Probit regression (Hoff 6.3)

A panel study followed $n = 25$ married couples over a period of five years. One item of interest is the relationship between divorce rates and the various characteristics of the couples. For example, the researchers would like to model the probability of divorce as a function of age differential, recorded as the man's age minus the woman's age. The data can be found in the file \texttt{divorce.RData}. We will model these data with probit regression, in which a binary variable $Y_i$ is described in terms of an explanatory variable $x_i$ via the following latent variable model: 
\begin{align*}
    Z_i & = \beta x_i + \epsilon_i \\
    Y_i & = `r indicator()`_{(c, +\infty)}(Z_i),
\end{align*}
where $\beta$ and $c$ are unknown coefficients, $\epsilon_1, \dots, \epsilon_n `r simiid()` \normal{0}{1}$ and $`r indicator()`_{(c, +\infty)}(z) = 1$ if $z > c$ and equals zero otherwise. In the following, since the covariates $x_i$ are known, they will be treated as constants and so not explicitly written in the conditioning part.

## Point a.

Assuming $\beta \sim \normal{0}{\sigma_\beta^2}$, obtain the full conditional distribution $p(\beta \given y_{1:n}, z_{1:n}, c)$. 

\medskip

First of all let us write explicitly the conditional distributions which we can deduce from the text:

- $\forall i = 1, \dots, n$ we know $p(z_i \given \beta)$: 
    \begin{gather*}
        Z_i(\omega) \given \beta = \beta x_i + \epsilon_i(\omega) \sim \beta x_i + \normal{0}{1} \sim \normal{\beta x_i}{1} \implies Z_i \given \beta \sim \normal{\beta x_i}{1} \\
        \Downarrow \\
        p(z_i \given \beta) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (z_i - \beta x_i)^2};
    \end{gather*}
- $\forall i = 1, \dots, n$ we know $p(y_i \given c, z_i)$: 
    \begin{gather*}
        Y_i(\omega) = `r indicator()`_{(c, +\infty)}(Z_i(\omega)) = 
        \begin{cases}
            1 & \text{ if } Z_i(\omega) > c \\
            0 & \text{ otherwise}
        \end{cases} \\
        \Downarrow \\
        \begin{aligned}
            p(y_i) & = \prob{Y_i = y_i} = \prob{`r indicator()`_{(c, +\infty)}(Z_i) = y_i} = \\
            & = \begin{cases}
                    \prob{`r indicator()`_{(c, +\infty)}(Z_i) = 1} & \text{ if } y_i = 1 \\
                    \prob{`r indicator()`_{(c, +\infty)}(Z_i) = 0} & \text{ if } y_i = 0 \\
                    0 & \text{ otherwise}
                \end{cases} = \\
            & = \begin{cases}
                    \prob{\{Z_i > c\}} & \text{ if } y_i = 1 \\
                    \prob{\{Z_i > c\}^C} & \text{ if } y_i = 0 \\
                    0 & \text{ otherwise}
                \end{cases} = \\
            & = \left(y_i \prob{\{Z_i > c\}} + (1 - y_i) \prob{\{Z_i > c\}^C}\right) `r indicator()`_{\{0, 1\}}(y_i),
        \end{aligned}
    \end{gather*}
    hence $Y_i \sim \bernoulli{\prob{Z_i > c}}$. `r newline()`
    It follows that, conditionally on $Z_i, c$, the r.v. $Y_i$ is no more `r textit("random")` and it holds[^1]
    \[
        p(y_i \given c, z_i) = \left(y_i `r indicator()`_{(-\infty, z_i)}(c) + (1 - y_i) `r indicator()`_{(-\infty, z_i)^C}(c)\right) `r indicator()`_{\{0, 1\}}(y_i).
    \]

    [^1]: We replace $\prob{\{z_i > c\}}$ with $`r indicator()`_{(-\infty, z_i)}(c)$ because we will use this characterization afterwards.

In order to obtain (and sample) from the full conditionals we assume $\beta$ and $c$ a priori independent. `r newline()`
The full conditional distribution $p(\beta \given y_{1:n}, z_{1:n}, c)$ can be obtained just from $p(z_i \given \beta)$, indeed
    \begin{align*}
        p(\beta \given y_{1:n}, z_{1:n}, c) & = \frac{p(\beta, y_{1:n}, z_{1:n}, c)}{p(y_{1:n}, z_{1:n}, c)} \frac{p(\beta, z_{1:n}, c)}{p(\beta, z_{1:n}, c)} \frac{p(\beta, c)}{p(\beta, c)} \frac{p(c)}{p(c)} \propto \\
        & \propto \frac{p(\beta, y_{1:n}, z_{1:n}, c)}{p(\beta, z_{1:n}, c)} \frac{p(\beta, z_{1:n}, c)}{p(\beta, c)} \frac{p(\beta, c)}{p(c)} = \\
        & = p(y_{1:n} \given `r cancel("\\beta")`, c, z_{1:n}) p(z_{1:n} \given \beta, `r cancel("c")`) p(\beta \given `r cancel("c")`) \propto \\
        & \propto p(z_{1:n} \given \beta) p(\beta).
    \end{align*}
So we can write explicitly
    \begin{align*}
        p(\beta \given y_{1:n}, z_{1:n}, c) & \propto p(z_{1:n} \given \beta) p(\beta) = \\
        & = \prod_{i = 1}^n p(z_i \given \beta) p(\beta) \propto \\
        & \propto \exp{-\frac{1}{2} \sum_{i = 1}^{n} (z_i - \beta x_i)^2} \exp{-\frac{1}{2} \frac{1}{\sigma_\beta^2} \beta^2} = \\
        & = \exp{-\frac{1}{2} \left(\beta^2 \sum_{i = 1}^{n} x_i^2 + `r cancel("\\sum_{i = 1}^{n} z_i^2")` - 2 \beta \sum_{i = 1}^{n} x_i z_i + \beta^2 \frac{1}{\sigma_\beta^2} \right)} = \\
        & = \operatorname{exp}\Bigg(- \underbrace{\left(\sum_{i = 1}^{n} x_i^2 + \frac{1}{\sigma_\beta^2}\right)}_{\displaystyle \deq (\sigma_{\beta, n}^2)^{-1}} \frac{\beta^2}{2} + \underbrace{\left(\sum_{i = 1}^{n} x_i z_i \right)}_{\displaystyle \deq \frac{\mu_{\beta, n}}{\sigma_{\beta, n}^2}} \beta \Bigg) ,
    \end{align*}
where from the 1\ts{st} to the 2\ts{nd} line we used $\left(Z_i \given \beta\right)_{i = 1}^n$ independent, identically distributed r.v.'s. `r newline()`
So we can conclude that 
    \begin{gather*}
        \beta \given y_{1:n}, z_{1:n}, c \sim \normal{\mu_{\beta, n}}{\sigma_{\beta, n}^2} \text{ with } 
        \begin{cases}
            \sigma_{\beta, n}^2 = \left(\sum_{i = 1}^{n} x_i^2 + \frac{1}{\sigma_\beta^2}\right)^{-1} \\
            \mu_{\beta, n} = \sigma_{\beta, n}^2 \left(\sum_{i = 1}^{n} x_i z_i \right)
        \end{cases} \\
        \Downarrow \\
        p(\beta \given y_{1:n}, z_{1:n}, c) = \frac{1}{\sqrt{2\pi\sigma_{\beta, n}^2}} \exp{-\frac{1}{2 \sigma_{\beta, n}^2} (\beta - \mu_{\beta, n})^2}.
    \end{gather*}
\qed

## Point b.

Assuming $c \sim \normal{0}{\sigma_c^2}$, show that $p(c \given y_{1:n}, z_{1:n}, \beta)$ is a constrained normal density, i.e. proportional to a normal density but constrained to lie in an interval. Similarly, show that $p(z_i \given y_{1:n}, z_{-i}, \beta, c)$ is proportional to a normal density but constrained to be either above $c$ or below $c$, depending on $y_i$. `r newline()`
`r textbf("Hint:")` A constrained, or truncated, normal random variable $V$ is obtained by restricting a normally distributed random variable $\normal{\mu}{\tau^2}$ to lie in an interval $(a, b)$, with possibly $a = -\infty$ or $b = +\infty$. We use the notation $V \sim \truncnormal{(a,b)}{\mu}{\tau^2}$. It holds:

- $p(v \given \mu, \tau^2, a, b) = \frac{1}{C} \frac{1}{\sqrt{2\pi\tau^2}} \exp{-\frac{1}{2\tau^2}(v - \mu)^2} `r indicator()`_{(a, b)}(v)$, where $C = \Phi\left(\frac{b - \mu}{\tau}\right) - \Phi\left(\frac{a - \mu}{\tau}\right)$ being $\Phi(\cdot)$ the cdf of the standard normal distribution. By definition, it holds $\Phi\left(\frac{b - \mu}{\tau}\right) = 1$ if $b = \infty$ and $\Phi\left(\frac{a - \mu}{\tau}\right) = 0$ if $a = -\infty$.
- Sampling can be performed thanks to the function \texttt{rtruncnorm(n, a, b, mean, sd)} from the package \texttt{truncnorm} [https://cran.r-project.org/web/packages/truncnorm/truncnorm.pdf]. This function receives in input the number of desired samples ($n$) and the four parameters specifying the distribution of $V$: $a, b, \mu, \tau$. Pay attention that it takes as last inputs the mean $\mu$ and the standard deviation $\tau$ (not the variance $\tau^2$) of the un-truncated normal density.

\medskip

As before, the full conditional distribution $p(c \given y_{1:n}, z_{1:n}, \beta)$ can be obtained just from $p(y_i \given c, z_i)$, indeed
    \begin{align*}
		p(c \given y_{1:n}, z_{1:n}, \beta) & = \frac{p(c, y_{1:n}, z_{1:n}, \beta)}{p(y_{1:n}, z_{1:n}, \beta)} \frac{p(\beta, c, z_{1:n})}{p(\beta, c, z_{1:n})} \frac{p(c, \beta)}{p(c, \beta)} \frac{p(\beta)}{p(\beta)} \propto \\
        & \propto \frac{p(c, y_{1:n}, z_{1:n}, \beta)}{p(\beta, c, z_{1:n})} \frac{p(\beta, c, z_{1:n})}{p(c, \beta)} \frac{p(c, \beta)}{p(\beta)} = \\
        & = p(y_{1:n} \given `r cancel("\\beta")`, c, z_{1:n}) p(z_{1:n} \given \beta, `r cancel("c")`) p(c \given `r cancel("\\beta")`) \propto \\
        & \propto p(y_{1:n} \given c, z_{1:n}) p(c).
    \end{align*}
So we can write explicitly
    \begin{align*}
        p(c \given y_{1:n}, z_{1:n}, \beta) & \propto p(y_{1:n} \given c, z_{1:n}) p(c) = \\
        & = \prod_{i = 1}^n p(y_i \given c, z_i) p(c) \propto \\
        & \propto \exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} \prod_{i = 1}^n \left(y_i `r indicator()`_{(-\infty, z_i)}(c) + (1 - y_i) `r indicator()`_{(-\infty, z_i)^C}(c)\right) `r indicator()`_{\{0, 1\}}(y_i) = \\
		& = \exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} \prod_{{i = 1, \dots, n \given y_i = 1}} `r indicator()`_{(-\infty, z_i)}(c) \cdot \prod_{{i = 1, \dots, n \given y_i = 0}} `r indicator()`_{[z_i, +\infty)}(c) = \\
		& = \exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} `r indicator()`_{(-\infty, \, \min\left(z_i \given i \in \{1, \dots, n\}, y_i = 1\right))}(c) `r indicator()`_{[\max\left(z_i \given i \in \{1, \dots, n\}, y_i = 0\right), \, +\infty)}(c),
    \end{align*}
where from the 1\ts{st} to the 2\ts{nd} line we used $\left(Y_i \given c, z_i\right)_{i = 1}^n$ independent, identically distributed r.v.'s. `r newline()`
More compactly, defining 
	\begin{align*}
		a_n & \deq \max\left(z_i \given i \in \{1, \dots, n\}, y_i = 0\right) \text{ and } \\
		b_n & \deq \min\left(z_i \given i \in \{1, \dots, n\}, y_i = 1\right),
	\end{align*}
one has 
	\[
        p(c \given y_{1:n}, z_{1:n}, \beta) \propto \exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} `r indicator()`_{[a_n, b_n)}(c),
	\]
where, for a good definition, we are using $a_n < b_n$ which is clearly true because, if $a_n, b_n$ are finite, $\forall i, j \in \{1, \dots, n\}$ such that $y_i = 0$, $y_j = 1$, $(-\infty, c] \ni z_i < z_j \in (c, +\infty)$. `r newline()`
First of all we have to observe that the indicator function constrains $c \in [a_n, b_n)$, but it is equivalent to $c \in (a_n, b_n)$ because our $p(c \given y_{1:n}, z_{1:n}, \beta)$ is a density function with respect to the lebesgue measure on $\real$ so each point has measure $0$ (so does $\{a_n\}$). `r newline()`
Then, let us observe that this conditional density is proportional to the kernel of a gaussian (evaluated in $c$) multiplied by an indicator function (also evaluated in $c$), which constrains the domain to an interval (not necessarily limited, possibly $a_n = -\infty$ or $b_n = + \infty$). `r newline()`
So completing the function $\exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} `r indicator()`_{(a_n, b_n)}(c)$ to a density one obtains
	\begin{gather*}
		p(c \given y_{1:n}, z_{1:n}, \beta) = \frac{1}{\displaystyle \Phi\left(\frac{b_n}{\sigma_c}\right) - \Phi\left(\frac{a_n}{\sigma_c}\right)} \frac{1}{\sqrt{2 \pi \sigma_c^2}} \exp{-\frac{1}{2} \frac{1}{\sigma_c^2} c^2} `r indicator()`_{(a_n, b_n)}(c) \\
		\Downarrow \\
		c \given y_{1:n}, z_{1:n}, \beta \sim \truncnormal{(a_n, b_n)}{0}{\sigma_c^2}.
	\end{gather*}

\smallskip 

Similarly
	\begin{align*}
		p(z_i \given y_{1:n}, z_{-i}, \beta, c) & = \frac{p(z_i, y_{1:n}, z_{-i}, \beta, c)}{p(y_{1:n}, z_{-i}, \beta, c)} \frac{p(z_i, z_{-i}, \beta, c)}{p(z_i, z_{-i}, \beta, c)} \frac{p(z_i, \beta, c)}{p(z_i, \beta, c)} \frac{p(\beta, c)}{p(\beta, c)} \propto \\
		& \propto \frac{p(z_i, y_{1:n}, z_{-i}, \beta, c)}{p(z_i, z_{-i}, \beta, c)} \frac{p(z_i, z_{-i}, \beta, c)}{p(z_i, \beta, c)} \frac{p(z_i, \beta, c)}{p(\beta, c)} = \\
		& = p(y_{1:n} \given z_{1:n}, `r cancel("\\beta")`, c) p(z_{-i} \given `r cancel("z_i")`, \beta, `r cancel("c")`) p(z_i \given \beta, `r cancel("c")`) \propto \\
		& \propto p(y_{1:n} \given z_{1:n}, c) p(z_i \given \beta) \propto \\
		& \propto \prod_{j = 1}^n p(y_j \given z_j, c) p(z_i \given \beta) \propto \\
		& \propto p(y_i \given z_i, c) p(z_i \given \beta) \propto \\
		& \propto \Big(y_i \underbrace{`r indicator()`_{(-\infty, z_i)}(c)}_{\displaystyle = `r indicator()`_{(c, +\infty)}(z_i)} + (1 - y_i) \underbrace{`r indicator()`_{(-\infty, z_i)^C}(c)}_{\displaystyle = `r indicator()`_{(-\infty, c]}(z_i)}\Big) `r indicator()`_{\{0, 1\}}(y_i) \exp{-\frac{1}{2} (z_i - \beta x_i)^2} = \\
		& = 
			\begin{cases}
				\displaystyle `r indicator()`_{(c, +\infty)}(z_i) \exp{-\frac{1}{2} (z_i - \beta x_i)^2} & \text{ if } y_i = 1 \\
				\displaystyle `r indicator()`_{(-\infty, c]}(z_i) \exp{-\frac{1}{2} (z_i - \beta x_i)^2} & \text{ if } y_i = 0 \\
			\end{cases}.
	\end{align*}
As before, this conditional density is proportional to the kernel of a gaussian (evaluated in $z_i$) multiplied by an indicator function (also evaluated in $z_i$) which constrains the domain to be $(c, +\infty)$ or $(-\infty, c]$ (equivalently $(-\infty, c)$, with the same motivation given above) depending on $y_i$. `r newline()`
In particular, completing to a density what we found
	\begin{gather*}
		p(z_i \given y_{1:n}, z_{-i}, \beta, c) = 
		\begin{cases}
			\displaystyle \frac{1}{1 - \Phi(c - x_i\beta)} \frac{1}{\sqrt{2 \pi}} \exp{-\frac{1}{2} (z_i - \beta x_i)^2} `r indicator()`_{(c, +\infty)}(z_i) & \text{ if } y_i = 1 \\
			\displaystyle \frac{1}{\Phi(c - x_i\beta)} \frac{1}{\sqrt{2 \pi}} \exp{-\frac{1}{2} (z_i - \beta x_i)^2} `r indicator()`_{(-\infty, c)}(z_i) & \text{ if } y_i = 0 \\
		\end{cases} \\
		\Downarrow \\
		Z_i \given y_{1:n}, z_{-i}, \beta, c \sim 
		\begin{cases}
			\displaystyle \truncnormal{(c, +\infty)}{\beta x_i}{1} \text{ if } y_i = 1 \\
			\displaystyle\truncnormal{(-\infty, c)}{\beta x_i}{1} \text{ if } y_i = 0 \\
		\end{cases}.
	\end{gather*}
\qed

## Point c.

Letting $\sigma_\beta^2 = \sigma_c^2 = 16$, implement a Gibbs sampling scheme that approximates the joint posterior distribution of $Z_{1:n}, \beta$ and $c$. After a burning of $1000$, run the Gibbs sampler long enough so that the effective sample sizes of all unknown parameters are greater than $1000$ (including the $Z_i$'s).
Compute the autocorrelation function of the parameters and discuss the mixing of the Markov chain.

\medskip

The prior distributions of $\beta$ and $c$ are 
\begin{align*}
	& \beta \sim \normal{0}{\sigma_\beta^2}, \\
	& c \sim \normal{0}{\sigma_c^2}.
\end{align*}
The full conditional distributions we found are the following
\begin{align*}
	& Z_i \given y_{i}, \beta, c \sim 
		\begin{cases}
			\displaystyle \truncnormal{(c, +\infty)}{\beta x_i}{1} \text{ if } y_i = 1 \\
			\displaystyle\truncnormal{(-\infty, c)}{\beta x_i}{1} \text{ if } y_i = 0 \\
		\end{cases} \ , \\
	& \beta \given z_{1:n} \sim \normal{\mu_{\beta, n}}{\sigma_{\beta, n}^2} \text{ with } 
        \begin{cases}
            \displaystyle \sigma_{\beta, n}^2 = \left(\sum_{i = 1}^{n} x_i^2 + \frac{1}{\sigma_\beta^2}\right)^{-1} \\
            \displaystyle \mu_{\beta, n} = \sigma_{\beta, n}^2 \left(\sum_{i = 1}^{n} x_i z_i \right) \\
		\end{cases} \ , \\
	& c \given y_{1:n}, z_{1:n} \sim \truncnormal{(a_n, b_n)}{0}{\sigma_c^2} \text{ with }
		\begin{cases}
			\displaystyle a_n & \deq \max\left(z_i \given i \in \{1, \dots, n\}, y_i = 0\right) \text{ and } \\
			\displaystyle b_n & \deq \min\left(z_i \given i \in \{1, \dots, n\}, y_i = 1\right)
		\end{cases}.
\end{align*}

<!-- load(file = "./2_hw_bs/rmd/divorce.RData") -->
\scriptsize
```{r}
library(coda)
library(truncnorm)
library(bayesplot)
library(dplyr)
library(ggplot2)
library(grid)
library(gridExtra)
library(lattice)

load(file = "divorce.RData")
set.seed(1)

# setting parameters
burnin = 1e3
tmax = burnin + 1e5
n = 25

# build and upload: beta, c, z_1:n, y_1:n
beta = c = matrix(0, tmax, 1)
z = y = matrix(0, tmax, n)
x = matrix(0, 1, n)
x[1, ] = divorce[, "X"]
y[1, ] = divorce[, "Y"]

# parameters for priors and full conditionals distributions
mu_beta = matrix(0, tmax, 1)
mu_beta[1] = 0
mu_c = 0
sigma_sq_beta = sigma_sq_c = 16
sigma_sq_beta_n = (sum(x^2) + (sigma_sq_beta)^(-1))^(-1)
a = matrix(-Inf, tmax, 1)
b = matrix(Inf, tmax, 1)

# prior samples
beta[1] = rnorm(1, mu_beta[1], sqrt(sigma_sq_beta))
c[1] = rnorm(1, mu_c, sqrt(sigma_sq_c))

# gibbs sampler
for (t in 2:(tmax)) {
	# z
	lower_bound = rep(c[t - 1], n)
	lower_bound[y[t - 1, ] == 0] = -Inf
	upper_bound = rep(c[t - 1], n)
	upper_bound[y[t - 1, ] == 1] = +Inf
	z[t, ] = rtruncnorm(n, lower_bound, upper_bound, beta[t - 1] * x, rep(1, n))

	# update y (redundant because they follow the behaviour of z which is sampled given y)
	y[t, ] = 1 * (z[t, ] > c[t - 1])

	# beta
	mu_beta[t] = sigma_sq_beta_n * sum(x * z[t, ])
	beta[t] = rnorm(1, mu_beta[t], sqrt(sigma_sq_beta_n))

	# c
	a[t] = max(z[t, ][y[t, ] == 0])
	b[t] = min(z[t, ][y[t, ] == 1])
	c[t] = rtruncnorm(1, a[t], b[t], mu_c, sqrt(sigma_sq_c))

	# re-update y (redundant because they follow the behaviour of z which is sampled given y)
	y[t, ] = 1 * (z[t, ] > c[t])

	# break if eff_size > 1000 (for all params)
	if (t > burnin + 1 & (t %% 1000 == 0)) { 		
		if (effectiveSize(c[c((burnin + 1):t)]) > 1000 &
			effectiveSize(beta[c((burnin + 1):t)]) > 1000 &
			prod(effectiveSize(z[c((burnin + 1):t), ]) > 1000) == 1) 
		{
			c = c[c((burnin + 1):t)]
			beta = beta[c((burnin + 1):t)]
			z = z[c((burnin + 1):t), ]
			break
		}
	}
}

# mcmc
params_mcmc = mcmc(cbind(beta, c, z))
```
\normalsize

\medskip

The effective sample size of the parameters are

- $S_{eff}(c) = `r round(effectiveSize(c), 2)`\,$;
- $S_{eff}(\beta) = `r round(effectiveSize(beta), 2)`\,$; 
- $S_{eff}(Z_{1:25}) = 
	\begin{aligned} 
		& `r paste(c("[", paste(round(matrix(effectiveSize(z), nrow = 1)[1:9], 2), collapse = ", ")), collapse = "")` \\
		& `r paste(c("\\ ", paste(round(matrix(effectiveSize(z), nrow = 1)[10:18], 2), collapse = ", ")), collapse = "")` \\
		& `r paste(c("\\ ", paste(round(matrix(effectiveSize(z), nrow = 1)[1:9], 2), collapse = ", "), "]"), collapse = "")` \\
	\end{aligned} \quad$.

\medskip

Below are the autocorrelation functions of the parameters:
- $\beta$ and $c$ :
	\smallskip
	\scriptsize
	```{r, fig.align = "center", out.width = "75%"}
	color_scheme_set("green")
	# autocorrelation function of beta and c
	mcmc_acf(as.data.frame(params_mcmc), pars = c("beta", "c"), lags = 50, facet_args = list(nrow = 2, labeller = label_parsed)) +
		theme_bw() + xlab(" ")	  
	```
	\normalsize
- $Z_{1:25}$ :
	\smallskip
	\scriptsize
	```{r, fig.align = "center", out.width = "75%"}
	# autocorrelation function of z_1:25
	mcmc_acf(as.data.frame(params_mcmc), pars = c(paste("V", 3:8, sep = "")), lags = 50) +
		theme_bw()
	mcmc_acf(as.data.frame(params_mcmc), pars = c(paste("V", 9:14, sep = "")), lags = 50) +
		theme_bw()
	mcmc_acf(as.data.frame(params_mcmc), pars = c(paste("V", 15:20, sep = "")), lags = 50) +
		theme_bw()
	mcmc_acf(as.data.frame(params_mcmc), pars = c(paste("V", 21:27, sep = "")), lags = 50) +
		theme_bw()
	```
	\normalsize

\medskip

Discuss the mixing of the markov chain\dots

## Point d.

Obtain a $95\%$ posterior credible interval for $\beta$, as well as $\prob{\beta > 0 \given y_{1:n}}$.

# Question 2: Hierarchical Modeling

The file schools.RData gives weekly hours spent on homework for students sampled from eight different schools.
Obtain posterior distributions for the true means for the eight different schools using a hierarchical normal model
with the following prior parameters:
\begin{align*}
\mu_{0} = 7, \gamma^2_{0} = 5, \eta_{0} = 2, \tau^2_{0} = 10, \nu_{0} = 2, \sigma^2_{0} = 15.
\end{align*}

That is,
\begin{align*}
& y_{1, j}, . . . , y{n_{j}, j} | \theta_{j} , \sigma^2 `r simiid()` \normal{\theta_{j}}{\sigma^2}, j = 1, . . . , 8, \\
&  \theta_{1} , . . . , \theta_{8} | \mu, \tau^2  `r simiid()` \normal{\mu}{\tau^2}, \\
& \mu \sim \normal{\mu}{\gamma_{0}^2} , \quad 1/\tau^2 Gamma(\eta_{0}/2, \eta_{0}\tau_{0}^2/2), \quad 1/\sigma^2 Gamma(\nu_{0}/2, \nu_{0}\sigma_{0}^2/2)
\end{align*}

## Point a.
We implement a Gibbs sampling scheme that approximates the posterior distribution of ${\theta_{1:8}, \mu, \sigma^2, \tau^2}$. After a burning of $1000$, we run the Gibbs sampler long enough so that the effective sample sizes of all unknown parameters are greater than $1000$ (including the $theta_i$'s).
\scriptsize
```{r}
set.seed(1)

load(file = "schools.RData")
# Prior parameters
mu0 = 7
g20 = 5
t20 = 10
eta0 = 2
s20 = 15
nu0 = 2

# Number of schools. Y[, 1] are school ids
m = length(unique(Y[, 1]))

# Starting values - use sample mean and variance
sv = double(length = m)
ybar = double(length = m)
n = integer(length = m)
for (j in 1:m) {
  Y_j = Y[Y[, 1] == j, 2]
  ybar[j] = mean(Y_j)
  sv[j] = var(Y_j)
  n[j] = length(Y_j)
}

# Let initial theta estimates be the sample means
# Similarly, let initial values of sigma2, mu, and tau2 be "sample mean and variance"

theta = ybar
sigma2 = mean(sv)
mu = mean(theta)
tau2 = var(theta)

# MCMC
burnin = 1e3
S = 5e3
theta.mcmc = mcmc(matrix(0, nrow = S, ncol = m)) # declare them as mcmc objects

# Storing sigma, mu, theta
sigma2.mcmc = mcmc(rep(0, S))
mu.mcmc = mcmc(rep(0, S))
tau2.mcmc = mcmc(rep(0, S))

for (s in 1:(burnin+S)) {
  
  # Sample thetas
  for (j in 1:m) {
    vtheta = 1 / (n[j] / sigma2 + 1 / tau2)
    etheta = vtheta * (ybar[j] * n[j] / sigma2 + mu / tau2)
    theta[j] = rnorm(1, etheta, sqrt(vtheta))
  }
  
  # Sample sigma2
  nun = nu0 + sum(n)
  ss = nu0 * s20
  
  # Pool variance
  for (j in 1:m) {
    ss = ss + sum((Y[Y[, 1] == j, 2] - theta[j])^2)
  }
  sigma2 = 1 / rgamma(1, nun / 2, ss / 2)
  
  # Sample mu
  vmu = 1 / (m / tau2 + 1 /g20)
  emu = vmu * (m * mean(theta) / tau2 + mu0 / g20)
  mu = rnorm(1, emu, sqrt(vmu))
  
  # Sample tau2
  etam = eta0 + m
  ss = eta0 * t20 + sum((theta - mu)^2)
  tau2 = 1 / rgamma(1, etam / 2, ss / 2)
  
  # Store params
  if(s > burnin){
    theta.mcmc[s-burnin, ] = theta
    sigma2.mcmc[s-burnin] = sigma2
    mu.mcmc[s-burnin] = mu
    tau2.mcmc[s-burnin] = tau2
    
    if (s > burnin + 12 & (s %% 1000 == 0)) {
        if (effectiveSize(mu.mcmc) >= 1000 &
        	effectiveSize(tau2.mcmc) >= 1000 & 
        	effectiveSize(sigma2.mcmc) >= 1000 &
       		prod(effectiveSize(theta.mcmc) >= 1000) == 1)
        {
            break
        }
	}
  }
}
  
```
\normalsize

\medskip
The results are the following:
\smallskip
\scriptsize
```{r, fig.align = "center", out.width = "75%"}

#theta
color_scheme_set("blue")
p1 <- mcmc_trace(as.data.frame(theta.mcmc), pars = c(paste("V", 1:4, sep = "")), facet_args = list(nrow = 4, labeller = label_parsed))+
  ggtitle(expression(paste("Traceplots of ", theta[1:4])))+
  scale_y_continuous(breaks = c(4, 6, 8 ,10, 12, 14))
p2 <- mcmc_areas(as.data.frame(theta.mcmc), pars = c(paste("V", 1:4, sep = "")), prob = 0.95, 
  prob_outer = 1, 
  point_est = "mean") +
  ggtitle(expression(paste("Density plots of the posteriors for ", theta[1:4])))
grid.arrange(p1, p2, nrow = 1)

p1 <- mcmc_trace(as.data.frame(theta.mcmc), pars = c(paste("V", 5:8, sep = "")), facet_args = list(nrow = 4, labeller = label_parsed))+
  ggtitle(expression(paste("Traceplots of ", theta[5:8])))+
  scale_y_continuous(breaks = c(4, 6, 8 ,10, 12, 14))
p2 <- mcmc_areas(as.data.frame(theta.mcmc), pars = c(paste("V", 5:8, sep = "")), prob = 0.95, 
  prob_outer = 1,
  point_est = "mean") +
  ggtitle(expression(paste("Density plots of the posteriors for ", theta[5:8])))

grid.arrange(p1, p2, nrow = 1)

#comparison
theme_update(plot.title = element_text(hjust = 0.5))
color_scheme_set("purple")
q1 <- mcmc_trace(as.data.frame(mu.mcmc))+
  ggtitle(expression(mu))+
  yaxis_title(on = FALSE)
q2<- mcmc_areas(
  as.data.frame(mu.mcmc), 
  prob = 0.95,
  prob_outer = 1, 
  point_est = "mean")+
  ggtitle(expression(mu))+
  yaxis_text(on = FALSE)

color_scheme_set("red")
q3 <- mcmc_trace(as.data.frame(sigma2.mcmc))+
  ggtitle(expression(sigma^2))+
  yaxis_title(on = FALSE)
q4 <- mcmc_areas(
  as.data.frame(sigma2.mcmc), 
  prob = 0.95, 
  prob_outer = 1, 
  point_est = "mean")+
  ggtitle(expression(sigma^2))+
  yaxis_text(on = FALSE)

color_scheme_set("teal")
q5 <- mcmc_trace(as.data.frame(tau2.mcmc))+
  ggtitle(expression(tau^2))+
  yaxis_title(on = FALSE)
q6 <- mcmc_areas(
  as.data.frame(tau2.mcmc), 
  prob = 0.95, 
  prob_outer = 1, 
  point_est = "mean")+
  ggtitle(expression(tau^2))+
  yaxis_text(on = FALSE)

grid.arrange(q1, q2, q3, q4, q5, q6, nrow = 3, top=textGrob("Traceplots and density plots"))

S_mu = effectiveSize(mu.mcmc)
S_sigma2 = effectiveSize(sigma2.mcmc)
S_tau2 = effectiveSize(tau2.mcmc)
S_theta = effectiveSize(theta.mcmc)

```
\normalsize
\smallskip
As you can see, the chains span quite well and converge nicely, though not very rapidly.

In fact, we can observe that the effective sample size of the parameters are: `r newline()`
- $S_{eff}(\mu) = `r round(effectiveSize(mu.mcmc), 0)`\,$;`r newline()`
- $S_{eff}(\sigma^2) = `r round(effectiveSize(sigma2.mcmc), 0)`\,$; `r newline()`
- $S_{eff}(\tau^2) = `r round(effectiveSize(tau2.mcmc), 0)`\,$; `r newline()`
- $S_{eff}(\theta_{1:8}) = 
	\begin{aligned} 
		& `r paste(c("[", paste(round(matrix(effectiveSize(z), nrow = 1)[1:8], 0), collapse = ", "), "]"), collapse = "")` `r newline()`
	\end{aligned} \quad$.
which, compared to their length = `r s - burnin`, signifies a quite slow convergence.

## Point b.

As we can observe, the posterior densities are peaked around their means, which differs from the prior ones, though not by much: the data mitigate the effect of the priors, given that the mean will converge to the sample one for each of the parameters, but the original offset was not too large. What the data definitely affects is the variance, given that the posterior distributions are more pronouncedly peaked around their expectation value, so that we gain more certainty around each value. This is furthermore reflected on their quantile-based $95\%$ posterior credible intervals.

\scriptsize
```{r}
#quantile-based 95% confidence regions for the MCMC
alpha = 0.05

a_mu = summary(mu.mcmc, quantiles= c(alpha/2, 1-alpha/2))
a_tau = summary(tau2.mcmc, quantiles= c(alpha/2, 1-alpha/2))
a_sigma = summary(sigma2.mcmc, quantiles= c(alpha/2, 1-alpha/2))

```
\normalsize
\smallskip

The quantile-based $95\%$ posterior credible interval for $\mu, \tau^2$ and $\sigma^2$ are, respectively: `r newline()`
- $\mu$: [`r paste( a_mu$quantiles[1], ",", a_mu$quantiles[2])`] and the posterior mean is $\bar{\mu}$ = `r round(a_mu$statistics[1], 2)` `r newline()`
- $\tau^2$: [ `r paste( a_tau$quantiles[1], ",", a_tau$quantiles[2])`] and the posterior mean is $\bar{\tau}^2$ = `r round(a_tau$statistics[1], 2)` `r newline()`
- $\sigma^2$: [ `r paste( a_sigma$quantiles[1], ",", a_sigma$quantiles[2])`] and the posterior mean $\bar{\sigma}^2$ = `r round(a_sigma$statistics[1], 2)``r newline()`


\scriptsize
```{r, fig.align = "center", out.width = "75%"}
library(invgamma)

#density plots comparison
x1 = seq(mu0 - 2*g20,  mu0 + 2*g20, length = 200)
x2 = seq(0.00001,  60, length = 200)
x3 = seq(a_sigma$quantiles[1] - 10,  50, length = 200)
norm_vals = dnorm(x1, mu0, g20)
invG_tau = dinvgamma(x2, eta0/2, eta0*t20/2)
invG_sigma = dinvgamma(x3, nu0/2, nu0*s20/2)
priors = data.frame(x1, x2, x3, norm_vals, invG_sigma, invG_tau)

color_scheme_set("blue")
mcmc_dens(as.data.frame(mu.mcmc), alpha = 0.5)+
  xaxis_text(on = FALSE)+
  ggtitle(expression(paste("Posterior distribution of ", mu))) +
  geom_area(data = priors, aes(x= x1, y=norm_vals, col = "prior"),  fill = "red", alpha = 0.3, size = 0.8) +
  geom_vline(aes(xintercept = a_mu$statistics[1], col = "posterior"), size = 1)+
  geom_vline(aes(xintercept = mu0, col = "prior"), size = 1)+
  scale_color_manual(name = "Legend", values = c("prior" = "red", "posterior" = "blue"), 
    labels = c("prior distribution", "posterior distribution"))

mcmc_dens(as.data.frame(tau2.mcmc), alpha = 0.5)+
  xaxis_text(on = FALSE)+
  ggtitle(expression(paste("Posterior distribution of ", tau^2))) +
  geom_area(data = priors, aes(x= x2, y=invG_tau, col = "prior"), fill = "red", alpha = 0.3, size = 0.8) +
  geom_vline(aes(xintercept = a_tau$statistics[1], col = "posterior"), size = 1)+
  geom_vline(aes(xintercept = t20, col = "prior"), size = 1)+
  scale_color_manual(name = "Legend", values = c("prior" = "red", "posterior" = "blue"), 
    labels = c("prior distribution", "posterior distribution"))

mcmc_dens(as.data.frame(sigma2.mcmc), alpha = 0.5)+
  xaxis_text(on = FALSE)+
  ggtitle(expression(paste("Posterior distribution of ", sigma^2))) +
  geom_area(data = priors, aes(x= x3, y=invG_sigma, col = "prior"), fill = "red", alpha = 0.3, size = 0.8) +
  geom_vline(aes(xintercept = a_sigma$statistics[1], col = "posterior"), size = 1)+
  geom_vline(aes(xintercept = s20, col = "prior"), size = 1)+
  scale_color_manual(name = "Legend", values = c("prior" = "red", "posterior" = "blue"), 
    labels = c("prior distribution", "posterior distribution"))

```
\normalsize

## Point c.

\scriptsize
```{r, fig.align = "center", out.width = "75%"}

#posterior of R obtained through MC sampling
sample.r.post = rep(0, s)
sample.r.prior = rep(0, s)
sample.sigma2.prior = rinvgamma(s, eta0/2, eta0*t20/2)
sample.tau2.prior = rinvgamma(s, nu0/2, nu0*s20/2)

for (i in 1:s){
  sample.r.post[i] = tau2.mcmc[i, drop = TRUE] /(sigma2.mcmc[i, drop = TRUE] + tau2.mcmc[i, drop = TRUE] )
  sample.r.prior[i] = sample.tau2.prior[i]/(sample.tau2.prior[i] + sample.sigma2.prior[i])
}

sample_r = data.frame(sample.r.prior, sample.r.post)

sample_r %>% ggplot()+
  geom_density(aes(x = sample.r.prior, col = "prior"), fill = "red", alpha = 0.3, size = 0.8)+
  geom_density(aes(x = sample.r.post, col="posterior"), fill = "blue", alpha = 0.3, size = 0.8)+
  scale_color_manual(name = "Legend", values = c("prior" = "red", "posterior" = "blue"), 
    labels = c("prior distribution", "posterior distribution"))+
  ylab(" ") +
  xlab(" ") +
  ggtitle("Posterior density VS Prior density for parameter R ")
```
\normalsize

## Point d.

\scriptsize
```{r}
a_theta = summary(theta.mcmc)
post_means = a_theta$statistics[1:8, 1]

sample.theta9 =  rep(0, s)
sample.y7 =  rep(0, s)
sample.y9 =  rep(0, s)
  
for (i in 1:S){
  sample.theta9[i] = rnorm(1, mu.mcmc[i, drop = TRUE], sqrt(tau2.mcmc[i, drop = TRUE]))
  sample.y7[i] = rnorm(1, theta.mcmc[i, 7, drop = TRUE], sqrt(sigma2.mcmc[i, drop = TRUE]))
  sample.y9[i] = rnorm(1, sample.theta9[i], sqrt(sigma2.mcmc[i, drop = TRUE] + tau2.mcmc[i, drop = TRUE]))
}

mc_theta = sum(sample.theta9 > theta.mcmc[1:S, 7]) / S
mc_y = sum(sample.y9 > sample.y7)/S

```
\normalsize

The Monte Carlo estimates of the posterior probabilities of the two events are, respectively: `r newline()`
-$\mathbb{P}[\theta_{9} > \theta_{7}] = `r mc_theta`$, `r newline()`
-$\mathbb{P}[\tilde{Y}_{9} > \tilde{Y}_{7}] = `r mc_y`$.

## Point e.

By plotting the sample averages $\bar{y}_i$ against the posterior expectation values of the $\theta_i$, we observe that they appear to have a linear relation.
\scriptsize
```{r}
library(ggrepel)

mean_plt = data.frame(post_means, ybar, names = 1:8)

mean_plt %>% ggplot(aes(x=post_means, y = ybar))+
  geom_point( col = "green")+
  geom_line(col = "darkgreen", alpha = 0.5, size = 1)+
  geom_hline(yintercept = a_mu$statistics[1], col = "orange", size = 0.8)+
  ylab(expression(y[i])) +
  xlab(expression(paste(theta[i]))) +
  ggtitle(expression(paste("Posterior mean parameters ", theta[i], " VS sample means ", y[i])))+
  scale_color_discrete(guide = "none")+
  geom_label_repel(aes(label = names),
                  box.padding   = 0.35, 
                  point.padding = 0.5,
                  segment.color = 'grey50')

total.sample.mean = sum(n*ybar)/sum(n)

```

The total sample mean is `r round(total.sample.mean, 2)` and the posterior expectation of the Grand Mean is `round(a_mu$statistics[1], 2)`, which means that they only differ by `r round(abs(total.sample.mean - a_mu$statistics[1])/2 * 100, 2)`$\%$ of the former.